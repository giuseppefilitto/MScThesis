\documentclass{standalone}
\begin{document}
\subsection{Training and Segmentation}

Training was performed using \textsc{TensorFlow}\cite{Tensorflow} and \textsc{Segmentation-models} API\cite{segmentation_models}.
The core of the training process is the custom $\mathtt{DataGenerator}$, which provides data, split them into training and validation sets, pre-processes the images, and performs data augmentation on the training set.
Data augmentation consists of the random vertical or horizontal flip of the input and label images.
The peculiarity consists of the capability of working directly with DICOM input files.
The input and label images are taken directly by the $\mathtt{DataGenerator}$, providing the relative $\mathtt{source\_path}$ and $\mathtt{label\_path}$. They are the paths of the directories containing the relative DICOM input and label images.

\begin{lstlisting}[language = python, caption=Custom $\mathtt{DataGenerator}$ implementation]
import tensorflow as tf
import pyradiomics
    
class DataGenerator:

    def __init__(self, batch_size, source_path, label_path, aug=False, seed=123, validation_split=0., subset='training'):


        np.random.seed(seed)
        source_files = sorted(glob.glob(source_path + '/*.dcm'))
        source_files = np.asarray(source_files)


        labels_files = sorted(glob.glob(label_path + '/*.png'))
        labels_files = np.asarray(labels_files)

        assert source_files.size == labels_files.size


        source_files, labels_files = self.randomize(source_files, labels_files)

        idx = np.arange(0, source_files.size)
        np.random.shuffle(idx)

        self._source_trainfiles = source_files[idx[int(source_files.size * validation_split):]]
        self._labels_trainfiles = labels_files[idx[int(labels_files.size * validation_split):]]

        self._source_valfiles = source_files[idx[:int(source_files.size * validation_split)]]
        self._labels_valfiles = labels_files[idx[:int(labels_files.size * validation_split)]]

        self.subset = subset

        if self.subset == 'training':
            self._num_data = self._source_trainfiles.size
        elif self.subset == 'validation':
            self._num_data = self._source_valfiles.size

        self.aug = aug
        self._batch = batch_size
        self._cbatch = 0
        self._data, self._label = (None, None)

    @property
    def num_data(self):
        return self._num_data

    def randomize(self, source, label):

        random_index = np.arange(0, source.size)
        np.random.shuffle(random_index)
        source = source[random_index]
        label = label[random_index]

        return (source, label)


    def resize(self, img, lbl):

        height, width = img.shape[0], img.shape[1]

        if height != 512:
            img = cv2.resize(img, (512, 512))
            lbl = cv2.resize(lbl, (512, 512))
        else:
            img = img
            lbl = lbl

        return (img, lbl)

    def random_vflip(self, img, lbl):
        idx = np.random.uniform(low=0., high=1.)
        if idx > 0.5:
            return (cv2.flip(img, 0), cv2.flip(lbl, 0))
        else:
            return (img, lbl)

    def random_hflip(self, img, lbl):
        idx = np.random.uniform(low=0., high=1.)
        if idx > 0.5:
            return (cv2.flip(img, 1), cv2.flip(lbl, 1))
        else:
            return (img, lbl)

    def rescale(self, img):
        rescaled = cv2.normalize(img, dst=None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
        return rescaled

    def denoise(self, img):

        patch_kw = dict(patch_size=5, patch_distance=6)
        sigma_est = np.mean(estimate_sigma(img))
        denoised = denoise_nl_means(img, h=10 * sigma_est, sigma=sigma_est, fast_mode=True, **patch_kw)
        return denoised

    def gamma_correction(self, img, gamma=1.0):
        igamma = 1.0 / gamma
        imin, imax = img.min(), img.max()

        img_c = img.copy()
        img_c = ((img_c - imin) / (imax - imin)) ** igamma
        img_c = img_c * (imax - imin) + imin
        return img_c

    def __iter__(self):
        self._cbatch = 0
        return self

    def __next__(self):
        if self._cbatch + self._batch >= self._num_data:
            self._cbatch = 0
            self._source_trainfiles, self._labels_trainfiles = self.randomize(self._source_trainfiles, self._labels_trainfiles)
            self._source_valfiles, self._labels_valfiles = self.randomize(self._source_valfiles, self._labels_valfiles)

        if self.subset == 'training':
            c_sources = self._source_trainfiles[self._cbatch:self._cbatch + self._batch]
            c_labels = self._labels_trainfiles[self._cbatch:self._cbatch + self._batch]
        elif self.subset == 'validation':
            c_sources = self._source_valfiles[self._cbatch:self._cbatch + self._batch]
            c_labels = self._labels_valfiles[self._cbatch:self._cbatch + self._batch]

        # load the data

        images = [pydicom.dcmread(f).pixel_array for f in c_sources]
        labels = [cv2.imread(f, 0) for f in c_labels]


        # check size

        images, labels = zip(*[self.resize(im, lbl) for im, lbl in zip(images, labels)])

        # cast

        images = [self.rescale(im) for im in images]
        labels = [self.rescale(lbl) for lbl in labels]

        # denoise
        images = [self.denoise(im) for im in images]

        # gamma correction
        images = [self.gamma_correction(im, gamma=1.5) for im in images]

        if self.aug:

            # random horizontal flip
            images, labels = zip(*[self.random_hflip(im, lbl) for im, lbl in zip(images, labels)])

            # random vertical flip
            images, labels = zip(*[self.random_vflip(im, lbl) for im, lbl in zip(images, labels)])

        images = [im[..., np.newaxis] for im in images]
        labels = [lbl[..., np.newaxis] for lbl in labels]

        # to numpy

        images = np.array(images)
        labels = np.array(labels)

        self._cbatch += self._batch

        return (images, labels)

    
\end{lstlisting}
The model and the losses used for the training, come from \textsc{Segmentation-models} API.
In particular, the model consists of a U-net architecture using \textit{efficientnetb0} as
backbone encoder.
The metric instead, $\mathtt{dice\_coeff}$, has been implemented by using \textsc{TensorFlow} functions.

\begin{lstlisting}[language = python, caption=model implementation]
import tensorflow as tf
import segmentation_models as sm

BACKBONE = 'efficientnetb0'
model = sm.Unet(BACKBONE, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1), encoder_weights=None, activation='sigmoid')

optimizer =  'adam'
    
dice_loss = sm.losses.DiceLoss()
focal_loss = sm.losses.BinaryFocalLoss()
loss = dice_loss + (1 * focal_loss)

def dice_coef(y_true, y_pred):
    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])
    total = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])
    dice = tf.reduce_mean((2. * intersection + smooth) / (total + 1.))
    return dice

metrics = [ dice_coef ]

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

\end{lstlisting}
The prediction of the images is obtained using the function $\mathtt{predict\_images}$,
providing the $\mathtt{slices}$ and the trained model. 
The result is an array, like $\mathtt{slices}$, containing the relative prediction for each slice.
\begin{lstlisting}[language = python, caption=prediction function implementation]

def predict_images(slices, model, pre_processing=False, t=0.1):

    if pre_processing:
        prep_slices = pre_processing_data(slices)
    else:
        prep_slices = slices

    predicted_slices = np.zeros_like(prep_slices)

    for layer in range(slices.shape[0]):
        img = prep_slices[layer, ...]
        predicted_slices[layer, ...] = model.predict(img[np.newaxis, ...])[...]
        predicted_slices[layer, ...] = np.where(predicted_slices[layer, ...] <= 0.1, 0, predicted_slices[layer, ...])


    return predicted_slices


\end{lstlisting}



\end{document}